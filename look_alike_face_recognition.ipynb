{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n\nimport io # Input/Output Module\nimport os # OS interfaces\nimport time\nimport cv2 # OpenCV package\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\n\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\n\nfrom urllib import request # module for opening HTTP requests\nfrom matplotlib import pyplot as plt # Plotting library\nfrom matplotlib.offsetbox import OffsetImage, AnnotationBbox\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:32:32.350905Z","iopub.execute_input":"2022-04-12T21:32:32.351372Z","iopub.status.idle":"2022-04-12T21:32:34.272333Z","shell.execute_reply.started":"2022-04-12T21:32:32.351259Z","shell.execute_reply":"2022-04-12T21:32:34.27117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"width:100%; height:140px\">\n    <img src=\"https://www.kuleuven.be/internationaal/thinktank/fotos-en-logos/ku-leuven-logo.png/image_preview\" width = 300px, heigh = auto align=left>\n</div>\n\n\nKUL H02A5a Computer Vision: Group Assignment 1\n---------------------------------------------------------------\nStudent numbers: <span style=\"color:red\">r0912072, r0708519, u0142457, r4</span>.\n\nThe goal of this assignment is to explore more advanced techniques for constructing features that better describe objects of interest and to perform face recognition using these features. This assignment will be delivered in groups of 5 (either composed by you or randomly assigned by your TA's).\n\nIn this assignment you are a group of computer vision experts that have been invited to ECCV 2021 to do a tutorial about  \"Feature representations, then and now\". To prepare the tutorial you are asked to participate in a kaggle competition and to release a notebook that can be easily studied by the tutorial participants. Your target audience is: (master) students who want to get a first hands-on introduction to the techniques that you apply.\n\n---------------------------------------------------------------\nThis notebook is structured as follows:\n0. Data loading & Preprocessing\n1. Feature Representations\n2. Evaluation Metrics \n3. Classifiers\n4. Experiments\n5. Publishing best results\n6. Discussion\n\nMake sure that your notebook is **self-contained** and **fully documented**. Walk us through all steps of your code. Treat your notebook as a tutorial for students who need to get a first hands-on introduction to the techniques that you apply. Provide strong arguments for the design choices that you made and what insights you got from your experiments. Make use of the *Group assignment* forum/discussion board on Toledo if you have any questions.\n\nFill in your student numbers above and get to it! Good luck! \n\n\n<div class=\"alert alert-block alert-info\">\n<b>NOTE:</b> This notebook is just a example/template, feel free to adjust in any way you please! Just keep things organised and document accordingly!\n</div>\n\n<div class=\"alert alert-block alert-info\">\n<b>NOTE:</b> Clearly indicate the improvements that you make!!! You can for instance use titles like: <i>3.1. Improvement: Non-linear SVM with RBF Kernel.<i>\n</div>\n    \n---------------------------------------------------------------\n# 0. Data loading & Preprocessing\n\n## 0.1. Loading data\nThe training set is many times smaller than the test set and this might strike you as odd, however, this is close to a real world scenario where your system might be put through daily use! In this session we will try to do the best we can with the data that we've got! ","metadata":{"papermill":{"duration":0.022868,"end_time":"2021-03-08T07:57:06.382109","exception":false,"start_time":"2021-03-08T07:57:06.359241","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Input data files are available in the read-only \"../input/\" directory\n\ntrain = pd.read_csv(\n    '/kaggle/input/kul-h02a5a-computer-vision-ga1-2022/train_set.csv', index_col = 0)\ntrain.index = train.index.rename('id')\n\ntest = pd.read_csv(\n    '/kaggle/input/kul-h02a5a-computer-vision-ga1-2022/test_set.csv', index_col = 0)\ntest.index = test.index.rename('id')\n\n# read the images as numpy arrays and store in \"img\" column\ntrain['img'] = [cv2.cvtColor(np.load('/kaggle/input/kul-h02a5a-computer-vision-ga1-2022/train/train_{}.npy'.format(index), allow_pickle=False), cv2.COLOR_BGR2RGB) \n                for index, row in train.iterrows()]\n\ntest['img'] = [cv2.cvtColor(np.load('/kaggle/input/kul-h02a5a-computer-vision-ga1-2022/test/test_{}.npy'.format(index), allow_pickle=False), cv2.COLOR_BGR2RGB) \n                for index, row in test.iterrows()]\n  \n\ntrain_size, test_size = len(train),len(test)\n\n\"The training set contains {} examples, the test set contains {} examples.\".format(train_size, test_size)","metadata":{"papermill":{"duration":37.543619,"end_time":"2021-03-08T07:57:43.9495","exception":false,"start_time":"2021-03-08T07:57:06.405881","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-12T21:32:34.620776Z","iopub.execute_input":"2022-04-12T21:32:34.621434Z","iopub.status.idle":"2022-04-12T21:32:56.155419Z","shell.execute_reply.started":"2022-04-12T21:32:34.621392Z","shell.execute_reply":"2022-04-12T21:32:56.154455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Note: this dataset is a subset of the* [*VGG face dataset*](https://www.robots.ox.ac.uk/~vgg/data/vgg_face/).\n\n## 0.2. A first look\nLet's have a look at the data columns and class distribution.","metadata":{"papermill":{"duration":0.023377,"end_time":"2021-03-08T07:57:43.997466","exception":false,"start_time":"2021-03-08T07:57:43.974089","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# The training set contains an identifier, name, image information and class label\ntrain.head(1)","metadata":{"papermill":{"duration":3.315629,"end_time":"2021-03-08T07:57:47.336913","exception":false,"start_time":"2021-03-08T07:57:44.021284","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-12T21:32:56.157252Z","iopub.execute_input":"2022-04-12T21:32:56.157509Z","iopub.status.idle":"2022-04-12T21:32:59.435053Z","shell.execute_reply.started":"2022-04-12T21:32:56.157477Z","shell.execute_reply":"2022-04-12T21:32:59.434317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The test set only contains an identifier and corresponding image information.\ntest.head(1)","metadata":{"papermill":{"duration":3.283501,"end_time":"2021-03-08T07:57:50.644778","exception":false,"start_time":"2021-03-08T07:57:47.361277","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-12T21:32:59.436566Z","iopub.execute_input":"2022-04-12T21:32:59.436828Z","iopub.status.idle":"2022-04-12T21:33:02.750593Z","shell.execute_reply.started":"2022-04-12T21:32:59.436796Z","shell.execute_reply":"2022-04-12T21:33:02.749728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The class distribution in the training set:\ntrain.groupby('name').agg({'img':'count', 'class': 'max'})","metadata":{"papermill":{"duration":0.046628,"end_time":"2021-03-08T07:57:50.716317","exception":false,"start_time":"2021-03-08T07:57:50.669689","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-12T21:33:02.752263Z","iopub.execute_input":"2022-04-12T21:33:02.752626Z","iopub.status.idle":"2022-04-12T21:33:02.770385Z","shell.execute_reply.started":"2022-04-12T21:33:02.752594Z","shell.execute_reply":"2022-04-12T21:33:02.769439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that **Jesse is assigned the classification label 1**, and **Mila is assigned the classification label 2**. The dataset also contains 20 images of **look alikes (assigned classification label 0)** and the raw images. \n\n## 0.3. Preprocess data\n### 0.3.1 Example: HAAR face detector\nIn this example we use the [HAAR feature based cascade classifiers](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_objdetect/py_face_detection/py_face_detection.html) to detect faces, then the faces are resized so that they all have the same shape. If there are multiple faces in an image, we only take the first one. \n\n<div class=\"alert alert-block alert-info\"> <b>NOTE:</b> You can write temporary files to <code>/kaggle/temp/</code> or <code>../../tmp</code>, but they won't be saved outside of the current session\n</div>\n","metadata":{"papermill":{"duration":0.025108,"end_time":"2021-03-08T07:57:50.766719","exception":false,"start_time":"2021-03-08T07:57:50.741611","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class HAARPreprocessor():\n    \"\"\"Preprocessing pipeline built around HAAR feature based cascade classifiers. \"\"\"\n    \n    def __init__(self, path, face_size):\n        self.face_size = face_size\n        file_path = os.path.join(path, \"haarcascade_frontalface_default.xml\")\n        if not os.path.exists(file_path): \n            if not os.path.exists(path):\n                os.mkdir(path)\n            self.download_model(file_path)\n        \n        self.classifier = cv2.CascadeClassifier(file_path)\n  \n    def download_model(self, path):\n        url = \"https://raw.githubusercontent.com/opencv/opencv/master/data/\"\\\n            \"haarcascades/haarcascade_frontalface_default.xml\"\n        \n        with request.urlopen(url) as r, open(path, 'wb') as f:\n            f.write(r.read())\n            \n    def detect_faces(self, img):\n        \"\"\"Detect all faces in an image.\"\"\"\n        \n        img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        return self.classifier.detectMultiScale(\n            img_gray,\n            scaleFactor=1.2,\n            minNeighbors=5,\n            minSize=(30, 30),\n            flags=cv2.CASCADE_SCALE_IMAGE\n        )\n        \n    def extract_faces(self, img):\n        \"\"\"Returns all faces (cropped) in an image.\"\"\"\n        \n        faces = self.detect_faces(img)\n\n        return [img[y:y+h, x:x+w] for (x, y, w, h) in faces]\n    \n    def preprocess(self, data_row):\n        faces = self.extract_faces(data_row['img'])\n        \n        # if no faces were found, return None\n        if len(faces) == 0:\n            nan_img = np.empty(self.face_size + (3,))\n            nan_img[:] = np.nan\n            return nan_img\n        \n        # only return the first face\n        return cv2.resize(faces[0], self.face_size, interpolation = cv2.INTER_AREA)\n            \n    def __call__(self, data):\n        return np.stack([self.preprocess(row) for _, row in data.iterrows()]).astype(int)","metadata":{"papermill":{"duration":0.042776,"end_time":"2021-03-08T07:57:50.834913","exception":false,"start_time":"2021-03-08T07:57:50.792137","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-12T21:15:04.78986Z","iopub.execute_input":"2022-04-12T21:15:04.790368Z","iopub.status.idle":"2022-04-12T21:15:04.805564Z","shell.execute_reply.started":"2022-04-12T21:15:04.790329Z","shell.execute_reply":"2022-04-12T21:15:04.804489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Visualise**\n\nLet's plot a few examples.","metadata":{"papermill":{"duration":0.025332,"end_time":"2021-03-08T07:57:50.885849","exception":false,"start_time":"2021-03-08T07:57:50.860517","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# parameter to play with \nFACE_SIZE = (100, 100)\n\ndef plot_image_sequence(data, n, imgs_per_row=7, cmap=None):\n    n_rows = 1 + int(n/(imgs_per_row+1))\n    n_cols = min(imgs_per_row, n)\n\n    f,ax = plt.subplots(n_rows,n_cols, figsize=(10*n_cols,10*n_rows))\n    for i in range(n):\n        if n == 1:\n            ax.imshow(data[i], cmap=cmap)\n        elif n_rows > 1:\n            ax[int(i/imgs_per_row),int(i%imgs_per_row)].imshow(data[i], cmap=cmap)\n        else:\n            ax[int(i%n)].imshow(data[i], cmap=cmap)\n    plt.show()","metadata":{"papermill":{"duration":62.263517,"end_time":"2021-03-08T07:58:53.174859","exception":false,"start_time":"2021-03-08T07:57:50.911342","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-12T21:33:02.77216Z","iopub.execute_input":"2022-04-12T21:33:02.772844Z","iopub.status.idle":"2022-04-12T21:33:02.782851Z","shell.execute_reply.started":"2022-04-12T21:33:02.772793Z","shell.execute_reply":"2022-04-12T21:33:02.781961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#preprocessed data \npreprocessor = HAARPreprocessor(path = '../../tmp', face_size=FACE_SIZE)\n\ntrain_X, train_y = preprocessor(train), train['class'].values\ntest_X = preprocessor(test)","metadata":{"papermill":{"duration":62.263517,"end_time":"2021-03-08T07:58:53.174859","exception":false,"start_time":"2021-03-08T07:57:50.911342","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-12T19:27:56.645029Z","iopub.execute_input":"2022-04-12T19:27:56.645322Z","iopub.status.idle":"2022-04-12T19:28:58.39541Z","shell.execute_reply.started":"2022-04-12T19:27:56.64529Z","shell.execute_reply":"2022-04-12T19:28:58.393751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot faces of Michael and Sarah\n\nplot_image_sequence(train_X[train_y == 0], n=20, imgs_per_row=10)","metadata":{"papermill":{"duration":2.635787,"end_time":"2021-03-08T07:58:55.836611","exception":false,"start_time":"2021-03-08T07:58:53.200824","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-12T19:28:58.396572Z","iopub.execute_input":"2022-04-12T19:28:58.396801Z","iopub.status.idle":"2022-04-12T19:29:01.046207Z","shell.execute_reply.started":"2022-04-12T19:28:58.396772Z","shell.execute_reply":"2022-04-12T19:29:01.045052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot faces of Jesse\n\nplot_image_sequence(train_X[train_y == 1], n=30, imgs_per_row=10)","metadata":{"papermill":{"duration":3.840961,"end_time":"2021-03-08T07:58:59.72249","exception":false,"start_time":"2021-03-08T07:58:55.881529","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-12T19:29:01.047443Z","iopub.execute_input":"2022-04-12T19:29:01.048028Z","iopub.status.idle":"2022-04-12T19:29:05.378285Z","shell.execute_reply.started":"2022-04-12T19:29:01.047921Z","shell.execute_reply":"2022-04-12T19:29:05.377203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot faces of Mila\n\nplot_image_sequence(train_X[train_y == 2], n=30, imgs_per_row=10)","metadata":{"papermill":{"duration":3.910256,"end_time":"2021-03-08T07:59:03.703299","exception":false,"start_time":"2021-03-08T07:58:59.793043","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-12T19:29:05.380152Z","iopub.execute_input":"2022-04-12T19:29:05.380743Z","iopub.status.idle":"2022-04-12T19:29:09.631235Z","shell.execute_reply.started":"2022-04-12T19:29:05.380693Z","shell.execute_reply":"2022-04-12T19:29:09.628942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A lot of faces are not well detected. Let's look for an alternative","metadata":{}},{"cell_type":"markdown","source":"### 0.3.2 Multi-Task Cascaded Convolutional Neural Network (MTCNN)\nMTCNN is A framework developed by*Zhang et al. (2016)* [[ZHANG2016](https://ieeexplore.ieee.org/document/7553523)], and is a state-of-the-art (SOTA) algorithm for face detection. MTCNN consists of three stages of convolutional networks that are able to recognize faces and landmark locations such as eyes, nose. Similar to last section 0.3.1, the extracted faces are resized so that they all have the same shape. And if there are multiple faces in an image, we only take the first one. Moreover, if there is no face detected, a black image with the default size will be returned. We use it as an alternative to HAAR for its better performance (though much longer detection time).\n\n**First step, MTCNN can be installed through pip:**, \n","metadata":{}},{"cell_type":"code","source":"pip install mtcnn","metadata":{"execution":{"iopub.status.busy":"2022-04-12T19:30:02.871002Z","iopub.execute_input":"2022-04-12T19:30:02.871653Z","iopub.status.idle":"2022-04-12T19:30:16.483916Z","shell.execute_reply.started":"2022-04-12T19:30:02.871589Z","shell.execute_reply":"2022-04-12T19:30:16.482395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Construct MTCNN extractor**","metadata":{}},{"cell_type":"code","source":"# face detection for the 5 Celebrity Faces Dataset\nfrom PIL import Image\nfrom mtcnn.mtcnn import MTCNN\n \n# extract a single face from a given photograph\ndef extract_face(image, required_size=(160, 160)):\n    # convert to array\n#     image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    pixels = np.asarray(image)\n    \n    # Detect faces in the image\n    detector = MTCNN()\n    results = detector.detect_faces(pixels)\n    \n    if len(results) == 0: # if no face detected\n        nan_img = np.empty(required_size + (3,))\n        nan_img[:] = np.nan\n        return nan_img\n    \n    # extract the bounding box from the first face\n    x1, y1, width, height = results[0]['box']\n    # bug fix\n    x1, y1 = abs(x1), abs(y1)\n    x2, y2 = x1 + width, y1 + height\n    # extract the face\n    face = pixels[y1:y2, x1:x2]\n    \n    # resize image to the required size\n    image = Image.fromarray(face)\n    image = image.resize(required_size)\n    face_array = np.asarray(image)\n    return face_array\n \n# load images and extract faces for all images\ndef load_faces(images):\n    faces = list()\n    for image in tqdm(images):\n        # get face\n        face = extract_face(image)\n        faces.append(face)\n        \n    return np.asarray(faces, dtype='uint8')","metadata":{"execution":{"iopub.status.busy":"2022-04-12T19:30:37.040409Z","iopub.execute_input":"2022-04-12T19:30:37.040985Z","iopub.status.idle":"2022-04-12T19:30:37.050549Z","shell.execute_reply.started":"2022-04-12T19:30:37.04094Z","shell.execute_reply":"2022-04-12T19:30:37.049568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Preprocess the train and test dataset**","metadata":{}},{"cell_type":"code","source":"# # load train dataset\n# train_X, train_y = load_faces(train.loc[:, 'img']), train['class'].values\n# print(train_X.shape, train_y.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T19:31:05.164126Z","iopub.execute_input":"2022-04-12T19:31:05.164944Z","iopub.status.idle":"2022-04-12T19:31:05.169129Z","shell.execute_reply.started":"2022-04-12T19:31:05.164891Z","shell.execute_reply":"2022-04-12T19:31:05.168215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # load test dataset\n# test_X = load_faces(test['img'])\n# print(test_X.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T19:31:25.418876Z","iopub.execute_input":"2022-04-12T19:31:25.419157Z","iopub.status.idle":"2022-04-12T19:31:25.423385Z","shell.execute_reply.started":"2022-04-12T19:31:25.41913Z","shell.execute_reply":"2022-04-12T19:31:25.42235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Save the extracted results in a .npz file to save time in the following experiments**","metadata":{}},{"cell_type":"code","source":"# # Save faces\n# np.savez_compressed('mtcnn-faces-dataset.npz', train_X, train_y, test_X)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T19:31:44.824997Z","iopub.execute_input":"2022-04-12T19:31:44.825757Z","iopub.status.idle":"2022-04-12T19:31:51.310989Z","shell.execute_reply.started":"2022-04-12T19:31:44.825709Z","shell.execute_reply":"2022-04-12T19:31:51.310319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Load MTCNN faces**\n\nMuch quicker than running MTCNN each time","metadata":{}},{"cell_type":"code","source":"mtcnn_faces = np.load('../input/mtcnn-faces/mtcnn-faces-dataset.npz')\ntrain_X = mtcnn_faces['arr_0']\ntrain_y = mtcnn_faces['arr_1']\ntest_X = mtcnn_faces['arr_2']","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:33:05.54862Z","iopub.execute_input":"2022-04-12T21:33:05.548919Z","iopub.status.idle":"2022-04-12T21:33:08.486845Z","shell.execute_reply.started":"2022-04-12T21:33:05.548887Z","shell.execute_reply":"2022-04-12T21:33:08.485796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's have a look at the faces detected with MTCNN. It performs a bit better than HAAR but sometimes, no faces are detected or a face that does not correspond to the current class is detected instead. We will manually remove the wrong face samples so that it does not hinder the trained classifier.","metadata":{}},{"cell_type":"code","source":"# plot faces of Michael and Sarah\n\nplot_image_sequence(train_X[train_y == 0], n=20, imgs_per_row=10)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T19:31:55.770654Z","iopub.execute_input":"2022-04-12T19:31:55.770966Z","iopub.status.idle":"2022-04-12T19:32:02.483423Z","shell.execute_reply.started":"2022-04-12T19:31:55.77093Z","shell.execute_reply":"2022-04-12T19:32:02.481327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot faces of Jesse\n\nplot_image_sequence(train_X[train_y == 1], n=30, imgs_per_row=10)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T19:32:02.485065Z","iopub.execute_input":"2022-04-12T19:32:02.485324Z","iopub.status.idle":"2022-04-12T19:32:12.029108Z","shell.execute_reply.started":"2022-04-12T19:32:02.485295Z","shell.execute_reply":"2022-04-12T19:32:12.028312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot faces of Mila\n\nplot_image_sequence(train_X[train_y == 2], n=30, imgs_per_row=10)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T19:32:12.030149Z","iopub.execute_input":"2022-04-12T19:32:12.030506Z","iopub.status.idle":"2022-04-12T19:32:21.010032Z","shell.execute_reply.started":"2022-04-12T19:32:12.030477Z","shell.execute_reply":"2022-04-12T19:32:21.009257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 0.4. Store Preprocessed data (optional)\n<div class=\"alert alert-block alert-info\">\n<b>NOTE:</b> You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\". Feel free to use this to store intermediary results.\n</div>","metadata":{"papermill":{"duration":0.100995,"end_time":"2021-03-08T07:59:03.904684","exception":false,"start_time":"2021-03-08T07:59:03.803689","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# # save preprocessed data\n# prep_path = '/kaggle/working/prep_data/'\n# if not os.path.exists(prep_path):\n#     os.mkdir(prep_path)\n    \n# np.save(os.path.join(prep_path, 'train_X.npy'), train_X)\n# np.save(os.path.join(prep_path, 'train_y.npy'), train_y)\n# np.save(os.path.join(prep_path, 'test_X.npy'), test_X)\n\n# # load preprocessed data\n# prep_path = '/kaggle/working/prep_data/'\n# if not os.path.exists(prep_path):\n#     os.mkdir(prep_path)\n# train_X = np.load(os.path.join(prep_path, 'train_X.npy'))\n# train_y = np.load(os.path.join(prep_path, 'train_y.npy'))\n# test_X = np.load(os.path.join(prep_path, 'test_X.npy'))","metadata":{"papermill":{"duration":0.109823,"end_time":"2021-03-08T07:59:04.11528","exception":false,"start_time":"2021-03-08T07:59:04.005457","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-12T19:34:12.80203Z","iopub.execute_input":"2022-04-12T19:34:12.802447Z","iopub.status.idle":"2022-04-12T19:34:12.807972Z","shell.execute_reply.started":"2022-04-12T19:34:12.802408Z","shell.execute_reply":"2022-04-12T19:34:12.807045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Remove bad images to improve the discriminability of following classifiers**","metadata":{}},{"cell_type":"code","source":"# Filter out bad images manually (where no faces or faces of wrong class appear)\nindices_to_delete = [8, 28, 40, 50, 65, 49, 53, 70]\ntrain_X = np.delete(train_X, indices_to_delete, axis=0)\ntrain_y = np.delete(train_y, indices_to_delete, axis=0)\ntrain.drop(index=indices_to_delete, inplace=True)\ntrain_X.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:33:08.489611Z","iopub.execute_input":"2022-04-12T21:33:08.489966Z","iopub.status.idle":"2022-04-12T21:33:08.504707Z","shell.execute_reply.started":"2022-04-12T21:33:08.489916Z","shell.execute_reply":"2022-04-12T21:33:08.503683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we are ready to rock!","metadata":{"papermill":{"duration":0.100101,"end_time":"2021-03-08T07:59:04.315571","exception":false,"start_time":"2021-03-08T07:59:04.21547","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# 1. Feature Representations\nIn this section, the feature representation methods mainly consist of two parts: Traditional Machine Learning-based, NN-based. For traditioanl methods, there are two baselines: HOG feature extractor and PCA feature extractor; for NN-based methods, FaceNet is applied to generate the embeddings of images.\n\n## 1.0. Example: Identify feature extractor\nOur example feature extractor doesn't actually do anything... It just returns the input:\n$$\n\\forall x : f(x) = x.\n$$\n\nIt does make for a good placeholder and baseclass ;).","metadata":{"papermill":{"duration":0.100212,"end_time":"2021-03-08T07:59:04.516059","exception":false,"start_time":"2021-03-08T07:59:04.415847","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class IdentityFeatureExtractor:\n    \"\"\"A simple function that returns the input\"\"\"\n    \n    def transform(self, X):\n        return X\n    \n    def __call__(self, X):\n        return self.transform(X)","metadata":{"papermill":{"duration":0.108781,"end_time":"2021-03-08T07:59:04.725071","exception":false,"start_time":"2021-03-08T07:59:04.61629","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-12T21:33:08.777908Z","iopub.execute_input":"2022-04-12T21:33:08.778821Z","iopub.status.idle":"2022-04-12T21:33:08.783871Z","shell.execute_reply.started":"2022-04-12T21:33:08.77877Z","shell.execute_reply":"2022-04-12T21:33:08.783257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.1. Baseline 1: HOG feature extractor\nHOG stands for Histogram Of Gradients. The use of gradients in image recognition is useful, because edges and corners can be characterized by large gradients. If you would calculate the gradients of an image moslty edge information would be maintained.\n\nTo make this representation more compact we divide the image in patches and within each patch we calculate the distribution of the gradients by means of a histogram. Both the scale of the patch and the number of bins of the histogram are design choices. Especially the size of the patch should correspond to the size of the features you'd want to capture in the image.\n\nThese histograms are then normalized so that they don't depend too much on lighting conditions. This is done by moving a block that contains 4 histograms accross the image. All these normalized vectors are than concatenated into one big feature vector.","metadata":{"papermill":{"duration":0.134288,"end_time":"2021-03-08T07:59:04.959911","exception":false,"start_time":"2021-03-08T07:59:04.825623","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Let's illustrate the HOG method with some code. First we create a HOGFeatureExtractor class that is derived from the IdentityFeatureExtractor baseclass.","metadata":{}},{"cell_type":"code","source":"class HOGFeatureExtractor(IdentityFeatureExtractor):\n    \"\"\"TODO: this feature extractor is under construction\"\"\"\n    \n    # set parameters for HOGDescriptor\n    def __init__(self, **params):\n        self.params = params\n        self.hog = cv2.HOGDescriptor(**self.params)\n        \n    # calculate the extracted feature from img X\n    def transform(self, X):\n        hist = []\n        for index in range(X.shape[0]):\n            hist.append(self.hog.compute(X[index].astype('uint8')))\n        return np.array(hist)","metadata":{"papermill":{"duration":0.110122,"end_time":"2021-03-08T07:59:05.171171","exception":false,"start_time":"2021-03-08T07:59:05.061049","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-12T21:15:16.980791Z","iopub.execute_input":"2022-04-12T21:15:16.981102Z","iopub.status.idle":"2022-04-12T21:15:16.988194Z","shell.execute_reply.started":"2022-04-12T21:15:16.98107Z","shell.execute_reply":"2022-04-12T21:15:16.987277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we have to choose suitable parameters for HOG. \n- window size: size of the image in which we want to extract the featurevector. Corresponds to FACE_SIZE\n- cellSize: size of the patch accros which we calculate one histogram. Depends on the size of our features.\n- blockSize: size of the block that does the normalization.\n- blockStride: how much does one block move each iteration. Multiple of cellSize.\n- nbins: number of bins in the gradient. Good default = 9.\n- signedGradient: does the histogram differentiate between 0 to 180 degr and 0 and -180 degr. From empirical results it was concluded that choosing an unsigned gradient is a good default.","metadata":{}},{"cell_type":"code","source":"# features we want to characterize are things like noses, eyes, mouths, ... Looking at our dataset\n# a good choice for a feature_size could be FACE_SIZE/8\nFEATURE_SIZE = (20,20)\nNORM_SIZE = (40,40) # double the feature_size means four histograms will be normalized together.\nhog_params = {\n    '_winSize': FACE_SIZE,\n    '_cellSize': FEATURE_SIZE,\n    '_blockSize': NORM_SIZE,\n    '_blockStride': FEATURE_SIZE,    \n    '_nbins': 9,\n    '_signedGradient': False, # using unsigned gradients typically yields better results\n}","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:15:17.902379Z","iopub.execute_input":"2022-04-12T21:15:17.903026Z","iopub.status.idle":"2022-04-12T21:15:17.907687Z","shell.execute_reply.started":"2022-04-12T21:15:17.902979Z","shell.execute_reply":"2022-04-12T21:15:17.907044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's now extract the HOG feature from each of the faces in our training dataset. The resulting feature vector will be stored in the train dataframe.","metadata":{}},{"cell_type":"code","source":"hog_feat_extractor = HOGFeatureExtractor(**hog_params)\nhog_feat = hog_feat_extractor.transform(train_X.astype('uint8'))","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:15:18.667054Z","iopub.execute_input":"2022-04-12T21:15:18.667372Z","iopub.status.idle":"2022-04-12T21:15:18.740882Z","shell.execute_reply.started":"2022-04-12T21:15:18.667335Z","shell.execute_reply":"2022-04-12T21:15:18.739897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's have a look at an extracted feature vector. Each of the faces in our dataset are now characterized by such a vector.","metadata":{}},{"cell_type":"code","source":"print(hog_feat[0])\nprint(hog_feat[0].shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:15:19.758778Z","iopub.execute_input":"2022-04-12T21:15:19.759092Z","iopub.status.idle":"2022-04-12T21:15:19.766068Z","shell.execute_reply.started":"2022-04-12T21:15:19.759061Z","shell.execute_reply":"2022-04-12T21:15:19.765031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.1.1. t-SNE Plots\nOf course the value of these extracted features depends on how well they enable us to classify the data. Are same-class feature vectors similar enough and non-same-class feature vectors different enough?\n\nTo check this we'll make a t-SNE plot. This is a method that returns a 2D vector that represents the original highD vector. This then allows us to show distances between highD vectors in a 2D plot. The 2D versions of the HOG feature vectors are stored in the 'train' dataframe.","metadata":{"papermill":{"duration":0.100377,"end_time":"2021-03-08T07:59:05.372401","exception":false,"start_time":"2021-03-08T07:59:05.272024","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from sklearn.manifold import TSNE","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:15:20.740776Z","iopub.execute_input":"2022-04-12T21:15:20.741819Z","iopub.status.idle":"2022-04-12T21:15:20.746063Z","shell.execute_reply.started":"2022-04-12T21:15:20.741771Z","shell.execute_reply":"2022-04-12T21:15:20.745088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Set it's parameters:\n- init='pca': method can also start from random init, but the documentation tells us that a pca init is more globally stable.\n- learning_rate='auto': influences how well 2D version will represent the highD version. 'auto' is a good default.","metadata":{}},{"cell_type":"code","source":"tsne = TSNE(verbose=1, init='pca', learning_rate='auto')\ntsne_res = tsne.fit_transform(hog_feat)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:15:21.473041Z","iopub.execute_input":"2022-04-12T21:15:21.474089Z","iopub.status.idle":"2022-04-12T21:15:22.08122Z","shell.execute_reply.started":"2022-04-12T21:15:21.474038Z","shell.execute_reply":"2022-04-12T21:15:22.080436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns # just some matplotlib.plt-based plotting tool\n\ntrain['tsne-x'] = tsne_res[:,0]\ntrain['tsne-y'] = tsne_res[:,1]\nplt.figure(figsize=(10,6))\nsns.scatterplot(\n    x=\"tsne-x\", y=\"tsne-y\",\n    hue=\"class\",\n    palette=sns.color_palette(\"hls\", 3),\n    data=train,\n    legend=\"full\",\n    alpha=0.8\n)","metadata":{"papermill":{"duration":0.100308,"end_time":"2021-03-08T07:59:05.57403","exception":false,"start_time":"2021-03-08T07:59:05.473722","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-12T21:15:22.085279Z","iopub.execute_input":"2022-04-12T21:15:22.085881Z","iopub.status.idle":"2022-04-12T21:15:22.468342Z","shell.execute_reply.started":"2022-04-12T21:15:22.085842Z","shell.execute_reply":"2022-04-12T21:15:22.467474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.1.2. Discussion\n\nRemember: class 0 = 'lookalikes', class 1 = 'Jesse', class 2 = 'Mila'\n\nWe notice that 'Jesse' and 'Mila' are really well separated from each other! The downside however is that the lookalikes still mingle with the other classes. This signifies that the features extracted from the lookalikes still resemble the features from the main classes too much. Especially the 'Mila'-class gets mingled up with its lookalikes. \nIf we now would feed these vectors to for example a SVM-classifier this promises to lead to a decent result. Though not perfect these features can still be usefull when used in conjuntion with other methods.\n\nIt's important to note that for this method to work well all faces in the dataset should be scaled to the same size! Because the cellsize is a fixed value, the size of the core features in each face should be comparable.\n\nDifferences in lightning variations are now compensated by the normalization of the histograms.\n\nCompared to the grabbing task in the previous assignment this is a more flexible method. In the previous the grabbing was a result of thresholding certain values in the HSV-space. The thresholding thus only works for the specific lighting conditions of that video. This method however is more flexible, because of its inherent normalization.","metadata":{"papermill":{"duration":0.100596,"end_time":"2021-03-08T07:59:05.775686","exception":false,"start_time":"2021-03-08T07:59:05.67509","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## 1.2. Baseline 2: PCA feature extractor","metadata":{"papermill":{"duration":0.101426,"end_time":"2021-03-08T07:59:05.978236","exception":false,"start_time":"2021-03-08T07:59:05.87681","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Principle Component Analysis is meant for dimensionality reduction of high dimensional data into p features that maximise the variance/information that each contains while being orthogonal to each other. The p features or principal components are thus linearly not correlated to each other. PCA is often used for data visualization or to extract a limited set of meaningful features.","metadata":{}},{"cell_type":"markdown","source":"Choose whether you want to work on the training or testing set","metadata":{}},{"cell_type":"code","source":"IS_TEST = True\nif IS_TEST:\n    data_X = test_X\nelse:\n    data_X = train_X\n    data_y = train_y","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:33:19.286933Z","iopub.execute_input":"2022-04-12T21:33:19.28744Z","iopub.status.idle":"2022-04-12T21:33:19.292826Z","shell.execute_reply.started":"2022-04-12T21:33:19.287394Z","shell.execute_reply":"2022-04-12T21:33:19.291683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:33:20.438322Z","iopub.execute_input":"2022-04-12T21:33:20.43928Z","iopub.status.idle":"2022-04-12T21:33:20.443472Z","shell.execute_reply.started":"2022-04-12T21:33:20.439237Z","shell.execute_reply":"2022-04-12T21:33:20.442507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PCAFeatureExtractor(IdentityFeatureExtractor):\n    \"\"\"PCA feature extractor\"\"\"\n        \n    def __init__(self, n_components, verbose=0):\n        self.n_components = n_components\n        self.verbose = verbose\n        \n        # pca with full rank svd, no approximation\n        self.pca = PCA(n_components=self.n_components, svd_solver='full')\n        \n        self.img_shape = (10, 10)\n    \n    def transform(self, X):\n        # Transform an image into its characteristic principal components ranked by amount of explained variance/information\n        \n        # Transform the 2D matrix into a 1D vector by concatenating each row on the same dimension\n        self.shape = X.shape[1:]\n        X = X.reshape(X.shape[0], -1) \n        \n        # extract principal components\n        res = self.pca.fit_transform(X)\n        \n        if self.verbose:\n            print('Explained variation per principal component:\\n{}'.format(self.pca.explained_variance_ratio_))\n        return res\n        \n    def inverse_transform(self, X):\n        # Retrieve back an image from the principal components\n        res = np.dot(X, self.pca.components_[:X.shape[1]]) + self.pca.mean_\n        res = res.reshape(res.shape[0], *self.shape)\n        return res\n    \n    def get_eigenfaces(self):\n        # Eigenfaces are simply principal components reshaped to the size of the original image\n        return self.pca.components_.reshape(self.pca.components_.shape[0], *self.shape)\n    \n    def get_n_components(self):\n        # The number of principal components\n        return self.pca.n_components","metadata":{"papermill":{"duration":0.111032,"end_time":"2021-03-08T07:59:06.191215","exception":false,"start_time":"2021-03-08T07:59:06.080183","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-12T21:33:20.769022Z","iopub.execute_input":"2022-04-12T21:33:20.769989Z","iopub.status.idle":"2022-04-12T21:33:20.781961Z","shell.execute_reply.started":"2022-04-12T21:33:20.769928Z","shell.execute_reply":"2022-04-12T21:33:20.78069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_X_grayscale = np.array([cv2.cvtColor(data_X[i].astype(\"uint8\"), cv2.COLOR_RGB2GRAY) for i in range(data_X.shape[0])])\nsamples = data_X_grayscale # if we want to have gray scale images\n#samples = data_X.reshape(data_X.shape[0], -1) # if we instead want to keep the 3 color channels \n                                               # but with colors, there are too many dimensions in practice","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:33:21.254741Z","iopub.execute_input":"2022-04-12T21:33:21.255138Z","iopub.status.idle":"2022-04-12T21:33:21.348796Z","shell.execute_reply.started":"2022-04-12T21:33:21.255086Z","shell.execute_reply":"2022-04-12T21:33:21.347605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Initialize and apply the PCA extractor on the training set. The PCA extractor is only applied on the images and not the label data, since PCA is an unsupervised machine learning technique. \n\nThe PCA extractor uses singular value decomposition and is in this way able to reduce the dimensionality of the data.\n\nOur images are of shape $(100, 100, 3)$. PCA works only on 1D vectors. We thus decied to convert them into grayscale (reduces the 3 color channel into 1) to reduce the input dimension and make the computation more effective. There is indeed much lower information to retrieve from colors than from shapes, edges, corners, ... We then concatenate rows to create a 1D vector of shape $(100 * 100, )$ for each image.\n\nWe substract the images by the mean image of the dataset as we want to retrieve features that contain as much information/variance that could help to discriminate different images of the dataset (and thus as different as possible from the mean image of the dataset). \n\nWe rely mostly on the sklearn implementation of PCA for this task (except for reconstructing the image from as subset of principal components which is not possible in sklearn).\n\nWe should use SVD instead of EVD. EVD can only be applied to square matrices. And more importantly, every matrix has a SVD. On the other hand, not even every square matrix has an eigenvalue decomposition. ","metadata":{}},{"cell_type":"code","source":"pcaFeatureExtractor = PCAFeatureExtractor(n_components=30, verbose=1)\nsamples_pca = pcaFeatureExtractor.transform(samples)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:33:22.210534Z","iopub.execute_input":"2022-04-12T21:33:22.210842Z","iopub.status.idle":"2022-04-12T21:33:46.897218Z","shell.execute_reply.started":"2022-04-12T21:33:22.210809Z","shell.execute_reply":"2022-04-12T21:33:46.896147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Listed above are all the eigenvalues generated by the pca algorithm in decreasing order. As we can see, we only have non-zero eigenvalues.\nThe PCA algorithm constructs the covariance matrix, in order to determine the eigenvalues. Since we used the PCA algorithm with full svd, the covariance matrix will be full-rank and thus also positive definite. From this we can state that we have no zero eigenvalues.","metadata":{}},{"cell_type":"code","source":"print(f\"Total variance explained with {pcaFeatureExtractor.get_n_components()} components:\"\n      f\"{pcaFeatureExtractor.pca.explained_variance_ratio_.sum()}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:33:46.899419Z","iopub.execute_input":"2022-04-12T21:33:46.900365Z","iopub.status.idle":"2022-04-12T21:33:46.906415Z","shell.execute_reply.started":"2022-04-12T21:33:46.900303Z","shell.execute_reply":"2022-04-12T21:33:46.905388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plot what effect the number of principal components has on the overall variance.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15, 6))\nplt.plot(range(pcaFeatureExtractor.get_n_components()), pcaFeatureExtractor.pca.explained_variance_ratio_)\nplt.title(\"Scree Plot\")\nplt.xlabel(\"Principal Component ID\")\nplt.ylabel(\"Explained Variance\")\nplt.xticks(range(pcaFeatureExtractor.get_n_components()))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:33:46.907816Z","iopub.execute_input":"2022-04-12T21:33:46.90832Z","iopub.status.idle":"2022-04-12T21:33:47.251245Z","shell.execute_reply.started":"2022-04-12T21:33:46.908283Z","shell.execute_reply":"2022-04-12T21:33:47.250012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will now reconstruct the data from the principal component space to the original image space. Then, we can determine how far the reconstructed image is from the original image in terms of the mean absolute error.","metadata":{}},{"cell_type":"markdown","source":"Determine the average reconstruction loss.\n","metadata":{}},{"cell_type":"code","source":"reconstructed_images = np.zeros((samples.shape[0], samples_pca.shape[1], *samples.shape[1:3]))\naverage_reconstruction_loss = np.zeros((samples_pca.shape[1],))\nfor component_id in range(samples_pca.shape[1]):\n    reconstructed_images[:, component_id] = pcaFeatureExtractor.inverse_transform(samples_pca[:, 0:component_id+1])\n    average_reconstruction_loss[component_id] = np.mean(abs(samples - reconstructed_images[:, component_id]))","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:33:47.25411Z","iopub.execute_input":"2022-04-12T21:33:47.25452Z","iopub.status.idle":"2022-04-12T21:34:11.133287Z","shell.execute_reply.started":"2022-04-12T21:33:47.254468Z","shell.execute_reply":"2022-04-12T21:34:11.132442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plot the average reconstruction loss for each amount of principal components.","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(15, 6))\nplt.plot(average_reconstruction_loss)\nplt.title(\"Average Reconstruction Plot\")\nplt.xticks(np.arange(len(average_reconstruction_loss)))\nplt.ylabel(\"Average Reconstruction Loss\")\nplt.xlabel(\"# of Principal Components\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:34:11.134592Z","iopub.execute_input":"2022-04-12T21:34:11.13532Z","iopub.status.idle":"2022-04-12T21:34:11.451401Z","shell.execute_reply.started":"2022-04-12T21:34:11.135282Z","shell.execute_reply":"2022-04-12T21:34:11.450388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the scree plot (and the average reconstruction plot), we think it might be a good idea to only keep 7 principal components. 7 corresponds to the last significant \"elbow\" in the scree plot. The scree plot shows indeed a significant added share of explained variance if we take 7 components into account instead of 6 or 8. It yields a good balance between the ratio of explained variance (i.e. discriminative power of the features) and a reduction of the dimensions (i.e. reduction of the complexity). We also observe that the average reconstruction loss reduces significantly until 7 components and only very slowly for each newly added component. ","metadata":{}},{"cell_type":"markdown","source":"### 1.2.1. Eigenface Plots","metadata":{"papermill":{"duration":0.100881,"end_time":"2021-03-08T07:59:06.392861","exception":false,"start_time":"2021-03-08T07:59:06.29198","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### Eigenface plots","metadata":{}},{"cell_type":"markdown","source":"Each eigenface shows a form of deviation from the mean face in this dataset. Some depicts a variation in the hair, face shape, eyes, ... ","metadata":{}},{"cell_type":"code","source":"plot_image_sequence(pcaFeatureExtractor.get_eigenfaces(), n=samples_pca.shape[1], imgs_per_row=10, cmap='gray')","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:34:11.45269Z","iopub.execute_input":"2022-04-12T21:34:11.452943Z","iopub.status.idle":"2022-04-12T21:34:18.421696Z","shell.execute_reply.started":"2022-04-12T21:34:11.45291Z","shell.execute_reply":"2022-04-12T21:34:18.420392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From these eigenface plots we can clearly see a distinction between every principal component. The principal components are ordered in decreasing order of explained variance, so the difference between each eigenface plot to the mean face gets less and less significant.","metadata":{}},{"cell_type":"markdown","source":"### Reconstructed images with more and more principal components","metadata":{}},{"cell_type":"code","source":"reconstructed_images = np.zeros((samples_pca.shape[1], *samples.shape[1:3]))\nfor component_id in range(samples_pca.shape[1]):\n    reconstructed_images[component_id] = pcaFeatureExtractor.inverse_transform(samples_pca[0, 0:component_id+1].reshape(1, -1))\n    \nplot_image_sequence(reconstructed_images, n=samples_pca.shape[1], imgs_per_row=10, cmap='gray')","metadata":{"papermill":{"duration":0.100638,"end_time":"2021-03-08T07:59:06.595002","exception":false,"start_time":"2021-03-08T07:59:06.494364","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-12T21:34:18.423396Z","iopub.execute_input":"2022-04-12T21:34:18.423969Z","iopub.status.idle":"2022-04-12T21:34:26.106047Z","shell.execute_reply.started":"2022-04-12T21:34:18.423911Z","shell.execute_reply":"2022-04-12T21:34:26.10494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we reconstruct images with more and more principal components, the image starts to resemble the original person more and more.","metadata":{}},{"cell_type":"markdown","source":"### 1.2.2. Feature Space Plots","metadata":{"papermill":{"duration":0.101263,"end_time":"2021-03-08T07:59:06.797448","exception":false,"start_time":"2021-03-08T07:59:06.696185","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"We will now plot the faces on the axes defined by the 2 first principal components, i.e. the 2 components that retain the most amount of variance in the dataset and that should discriminate the faces the most","metadata":{}},{"cell_type":"code","source":"def scatterplot_with_images(ax, XY, images, image_zoom=1, cmap=None):\n    for xy, img in zip(XY, images):\n        x, y = xy\n        img = OffsetImage(img, zoom=image_zoom, cmap=cmap)\n        ab = AnnotationBbox(img, (x, y), xycoords='data', frameon=False)\n        ax.add_artist(ab)\n    ax.update_datalim(XY)\n    ax.autoscale()\n    return ax","metadata":{"papermill":{"duration":0.101801,"end_time":"2021-03-08T07:59:07.000598","exception":false,"start_time":"2021-03-08T07:59:06.898797","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-12T21:34:26.107469Z","iopub.execute_input":"2022-04-12T21:34:26.107734Z","iopub.status.idle":"2022-04-12T21:34:26.11415Z","shell.execute_reply.started":"2022-04-12T21:34:26.107702Z","shell.execute_reply":"2022-04-12T21:34:26.113517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15, 10))\nscatterplot_with_images(ax, samples_pca[:, [0, 1]], samples, image_zoom=0.3, cmap='gray')\nplt.xlabel('Principal Component 0 / Eigenface 0')\nplt.ylabel('Principal Component 1 / Eigenface 1')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:34:26.115434Z","iopub.execute_input":"2022-04-12T21:34:26.115867Z","iopub.status.idle":"2022-04-12T21:34:38.402418Z","shell.execute_reply.started":"2022-04-12T21:34:26.115834Z","shell.execute_reply":"2022-04-12T21:34:38.401232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We indeed can see that the first principal component/eigenface has a wider axis on which the faces are more widespread than on the second eigenface. But both together already allows to separate rougly the images per class although it is not perfect. We know indeed that the first 2 eigenfaces do not cover most of the variance of the dataset together which makes PCA less powerful than it can usually get on other practical problems. A sufficient number of eigenfaces could still be used as features fed to a classifier.","metadata":{}},{"cell_type":"markdown","source":"## 1.3. NN Feature Extractor\n\n## FaceNet (2015)\n\nFaceNet was proposed by Schroff et al. (2015) in the paper titled FaceNet: A Unified Embedding for Face Recognition and Clustering. It achieved state-of-the-art results in the many benchmark face recognition dataset such as Labeled Faces in the Wild (LFW) with 99.63% and Youtube Face Database with 95.12%. \n\nThe model structure is shown in the following figure:\n\n![Model Structure](https://storage.googleapis.com/kagglesdsdata/datasets/2073385/3441934/facenet.jpg?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20220411%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20220411T090344Z&X-Goog-Expires=345599&X-Goog-SignedHeaders=host&X-Goog-Signature=b437a26a0faa07b7ac57a44a8cac44ecac1ca083e839809d2812dee309b428a34277e7a0b3c594d465bd18a725da9e1c1fd47c8292e9f7b42e73f123c9297b8a4e7d05bc051419bc91637b8db722fdc6db176be9d81a90cb2a46ed6a995de5bb5561e94244b200a76f7d4df15c412eea69092748a3bbf6f6ec6f71c43947e7a4ed2b1695365c7a589c362eade6e644109de654bd90d63cab12f3e234dfac79cb3b16a1b1bf69fc6c0b0379c3d47853b2e9c87ffd81e16fa8067ac1ff0cb4eac8a474c4760cf2957ebd8630c47846eb988efae32d854890d110e10282b112042b6bdb92352e75a6da3f2aa5521e483938e4dfbcc19bbd44c4d890e568207bdc86)\n\nIt consists of a batch input layer and a deep CNN followed by L2 normalization, which results in the face embedding. To train FaceNet, the triplet loss function is applied, which encourages feature vectors for the same identity to become more similar, and in versa less similar. Through a high-quality face mapping from the images using using deep learning architectures such as ZF-Net and Inception, the embeddings can be created directly in the training phase. Based on the generated face embedding, the classifier systems to classify faces can be used.\n\nIn this tutorial, we will first use the pretrained Keras FaceNet model prtrained on MS-Celeb-1M dataset. Notably, the input images should be in RGB format, and their pixel values are also standardized across all thress channels, the input image size is 160x160 pixels. Furthermore, we will rely on transfer learning with the pretrained facenet model (which was trained for face classification) to retrieve very discriminative embeddings of our own images.","metadata":{}},{"cell_type":"code","source":"# Tensorflow and Keras\nfrom tensorflow.keras.models import load_model","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:34:38.406219Z","iopub.execute_input":"2022-04-12T21:34:38.406532Z","iopub.status.idle":"2022-04-12T21:34:44.063528Z","shell.execute_reply.started":"2022-04-12T21:34:38.406494Z","shell.execute_reply":"2022-04-12T21:34:44.062497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class NNFeatureExtractor(IdentityFeatureExtractor):\n    \n    def __init__(self, model_path, input_size = (160, 160), **params):\n\n        self.input_size = input_size\n        # Load pretrained model\n        self.model = load_model(model_path)\n        \n    def transform(self, X):\n        # Preprocess images\n        preprocessed_samples = []\n        for image in tqdm(X):\n            preprocessed_samples.append(self.preprocess(image))\n        preprocessed_samples = np.asarray(preprocessed_samples)\n        \n        # Retrieve embeddings\n        embeddings = self.model.predict(preprocessed_samples)\n        return embeddings\n    \n    # preprocess image\n    def preprocess(self, image):\n        # Convert cv2 image\n        image = np.array(image, dtype='uint8')\n        # Resize image to be adapted to the pretrained model input size\n        image = cv2.resize(image, self.input_size, interpolation = cv2.INTER_AREA)\n        # Convert back to tensor-friendly floats\n        image = image.astype('float32')\n        \n        # Standardize pixel values across channels \n        image = (image - image.mean()) / image.std()\n\n        return image","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:34:44.064933Z","iopub.execute_input":"2022-04-12T21:34:44.065172Z","iopub.status.idle":"2022-04-12T21:34:44.07587Z","shell.execute_reply.started":"2022-04-12T21:34:44.065143Z","shell.execute_reply":"2022-04-12T21:34:44.075235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMG_SIZE = (160, 160, 3)\nnn_params = {'model_path': '../input/facenet/keras-facenet/model/facenet_keras.h5', 'input_size': IMG_SIZE[:2]}\nnnFeatureExtractor = NNFeatureExtractor(**nn_params)\n# Extract the embeddings of the train images\ntrain_embeddings = nnFeatureExtractor.transform(train_X)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:34:44.07681Z","iopub.execute_input":"2022-04-12T21:34:44.077133Z","iopub.status.idle":"2022-04-12T21:34:54.549538Z","shell.execute_reply.started":"2022-04-12T21:34:44.077091Z","shell.execute_reply":"2022-04-12T21:34:54.54862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Evaluation Metrics\n## 2.0. Example: Accuracy\nAs example metric we take the accuracy. Informally, accuracy is the proportion of correct predictions over the total amount of predictions. It is used a lot in classification but it certainly has its disadvantages...","metadata":{"papermill":{"duration":0.10088,"end_time":"2021-03-08T07:59:07.406787","exception":false,"start_time":"2021-03-08T07:59:07.305907","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score","metadata":{"papermill":{"duration":1.180116,"end_time":"2021-03-08T07:59:08.688561","exception":false,"start_time":"2021-03-08T07:59:07.508445","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-12T21:34:54.550897Z","iopub.execute_input":"2022-04-12T21:34:54.551716Z","iopub.status.idle":"2022-04-12T21:34:54.556409Z","shell.execute_reply.started":"2022-04-12T21:34:54.551661Z","shell.execute_reply":"2022-04-12T21:34:54.555489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Classifiers\n## 3.0. Example: The *'not so smart'* classifier\nThis random classifier is not very complicated. It makes predictions at random, based on the distribution obseved in the training set. **It thus assumes** that the class labels of the test set will be distributed similarly to the training set.","metadata":{"papermill":{"duration":0.103749,"end_time":"2021-03-08T07:59:08.894358","exception":false,"start_time":"2021-03-08T07:59:08.790609","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class RandomClassificationModel:\n    \"\"\"Random classifier, draws a random sample based on class distribution observed \n    during training.\"\"\"\n    \n    def fit(self, X, y):\n        \"\"\"Adjusts the class ratio instance variable to the one observed in y. \n\n        Parameters\n        ----------\n        X : tensor\n            Training set\n        y : array\n            Training set labels\n\n        Returns\n        -------\n        self : RandomClassificationModel\n        \"\"\"\n        \n        self.classes, self.class_ratio = np.unique(y, return_counts=True)\n        self.class_ratio = self.class_ratio / self.class_ratio.sum()\n        return self\n        \n    def predict(self, X):\n        \"\"\"Samples labels for the input data. \n\n        Parameters\n        ----------\n        X : tensor\n            dataset\n            \n        Returns\n        -------\n        y_star : array\n            'Predicted' labels\n        \"\"\"\n\n        np.random.seed(0)\n        return np.random.choice(self.classes, size = X.shape[0], p=self.class_ratio)\n    \n    def __call__(self, X):\n        return self.predict(X)\n    ","metadata":{"papermill":{"duration":0.113194,"end_time":"2021-03-08T07:59:09.110222","exception":false,"start_time":"2021-03-08T07:59:08.997028","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-12T21:34:54.558231Z","iopub.execute_input":"2022-04-12T21:34:54.558775Z","iopub.status.idle":"2022-04-12T21:34:54.574617Z","shell.execute_reply.started":"2022-04-12T21:34:54.558724Z","shell.execute_reply":"2022-04-12T21:34:54.573848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.1. Classification Model\nOur classification model will be a SVM classifier with a linear kernel. It was the best-performing option out of our different experiments: K-NN classifiers, SVM's with other kernels. \n\nC=10 is the parameter responsible for regularization by l2 penalty. Given the very discriminative features retrieved from facenet, we experimented higher performance in lowering the weight of the regularization. This is done by increasing C. \n\nWe have access to the test image but not the test labels which allows for semi-supervised learning. We further improved our score a bit by adapting this SVM classifier for semi-supervised learning allowing unlabeled samples that achieve a prediction probability higher than 0.9 to be labeled to their predicted output for further iterations. This progressively increases the size of the effective train set with more and more labeled data. The threshold of 0.9 is high to ensure that only very confident predictions can serve as labels for further iterations. This is enforced given the very similar faces of Michael and Jess, and Sarah and Mila. We allow a maximum of 10 iterations over the full unlabeled data of this process of labeling by predicted outputs.","metadata":{"papermill":{"duration":0.101099,"end_time":"2021-03-08T07:59:09.31402","exception":false,"start_time":"2021-03-08T07:59:09.212921","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.semi_supervised import SelfTrainingClassifier","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:34:54.576207Z","iopub.execute_input":"2022-04-12T21:34:54.576755Z","iopub.status.idle":"2022-04-12T21:34:54.597318Z","shell.execute_reply.started":"2022-04-12T21:34:54.576703Z","shell.execute_reply":"2022-04-12T21:34:54.596329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ClassificationModel:\n    \"\"\"TODO: this classifier is under construction.\"\"\"\n    \n    def __init__(self):\n        svc = SVC(kernel='linear', C=10, probability=True, random_state=42)\n        self.cls = SelfTrainingClassifier(svc, threshold = 0.9, max_iter=10) # semi-supervised SVC classifier\n    \n    def fit(self, X, y):\n        self.cls.fit(X, y)\n        \n    def predict(self, X):\n        return self.cls.predict(X)\n    \n    def __call__(self, X):\n        return self.predict(X)","metadata":{"papermill":{"duration":0.108542,"end_time":"2021-03-08T07:59:09.525054","exception":false,"start_time":"2021-03-08T07:59:09.416512","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-12T21:34:54.599641Z","iopub.execute_input":"2022-04-12T21:34:54.600016Z","iopub.status.idle":"2022-04-12T21:34:54.609204Z","shell.execute_reply.started":"2022-04-12T21:34:54.599966Z","shell.execute_reply":"2022-04-12T21:34:54.608129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2. Feature Extractor","metadata":{}},{"cell_type":"markdown","source":"We tested different approach from concatenating different l2-normalized feature representations (from the HOG, PCA and NN feature extractors). Our best results came from using the NN feature extractor alone","metadata":{}},{"cell_type":"code","source":"class FeatureExtractor(IdentityFeatureExtractor):\n    \"\"\"TODO: this feature extractor is under construction\"\"\"\n    \n    def __init__(self, nn_params, verbose=0): \n        self.nn_feature_extractor = NNFeatureExtractor(**nn_params)\n    \n    def transform(self, X):\n        nn_feat = self.nn_feature_extractor.transform(X)\n        return np.nan_to_num(nn_feat)\n\n    def __call__(self, X):\n        return self.transform(X)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:34:54.610763Z","iopub.execute_input":"2022-04-12T21:34:54.611583Z","iopub.status.idle":"2022-04-12T21:34:54.624658Z","shell.execute_reply.started":"2022-04-12T21:34:54.61153Z","shell.execute_reply":"2022-04-12T21:34:54.623479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Experiments\n<div class=\"alert alert-block alert-info\"> <b>NOTE:</b> Do <i>NOT</i> use this section to keep track of every little change you make in your code! Instead, highlight the most important findings and the major (best) pipelines that you've discovered.  \n</div>\n<br>\n\n## 4.0. Pipeline\n","metadata":{"papermill":{"duration":0.102942,"end_time":"2021-03-08T07:59:09.730342","exception":false,"start_time":"2021-03-08T07:59:09.6274","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train_test_X = np.concatenate((train_X, test_X))\ntrain_test_y = np.concatenate((train_y, [-1]*len(test_X))) # -1 for unlabeled test data","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:34:54.626831Z","iopub.execute_input":"2022-04-12T21:34:54.627142Z","iopub.status.idle":"2022-04-12T21:34:54.715689Z","shell.execute_reply.started":"2022-04-12T21:34:54.627104Z","shell.execute_reply":"2022-04-12T21:34:54.714485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_extractor = FeatureExtractor(nn_params=nn_params)\nclassifier = ClassificationModel()\n\n# train the model on the features\n# Supervised learning\n# classifier.fit(feature_extractor(train_X), train_y)\n# Semi-supervised learning\nclassifier.fit(feature_extractor(train_test_X), train_test_y)\n\n# model/final pipeline\nmodel = lambda X: classifier(feature_extractor(X))","metadata":{"papermill":{"duration":0.430319,"end_time":"2021-03-08T07:59:10.263691","exception":false,"start_time":"2021-03-08T07:59:09.833372","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-12T21:34:54.717387Z","iopub.execute_input":"2022-04-12T21:34:54.718043Z","iopub.status.idle":"2022-04-12T21:35:56.600395Z","shell.execute_reply.started":"2022-04-12T21:34:54.717978Z","shell.execute_reply":"2022-04-12T21:35:56.599197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluate performance of the model on the training set\ntrain_y_star = model(train_X)\n\n\"The performance on the training set is {:.2f}. This however, does not tell us much about the actual performance (generalisability).\".format(\n    accuracy_score(train_y, train_y_star))","metadata":{"papermill":{"duration":0.114717,"end_time":"2021-03-08T07:59:10.480473","exception":false,"start_time":"2021-03-08T07:59:10.365756","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-12T21:35:56.602072Z","iopub.execute_input":"2022-04-12T21:35:56.602438Z","iopub.status.idle":"2022-04-12T21:36:01.630766Z","shell.execute_reply.started":"2022-04-12T21:35:56.602389Z","shell.execute_reply":"2022-04-12T21:36:01.629784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict the labels for the test set \ntest_y_star = model(np.nan_to_num(test_X))","metadata":{"papermill":{"duration":0.111828,"end_time":"2021-03-08T07:59:10.696438","exception":false,"start_time":"2021-03-08T07:59:10.58461","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-12T21:36:01.632334Z","iopub.execute_input":"2022-04-12T21:36:01.632963Z","iopub.status.idle":"2022-04-12T21:36:51.674419Z","shell.execute_reply.started":"2022-04-12T21:36:01.632909Z","shell.execute_reply":"2022-04-12T21:36:51.673572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Publishing best results","metadata":{"papermill":{"duration":0.103853,"end_time":"2021-03-08T07:59:10.903341","exception":false,"start_time":"2021-03-08T07:59:10.799488","status":"completed"},"tags":[]}},{"cell_type":"code","source":"submission = test.copy().drop('img', axis = 1)\nsubmission['class'] = test_y_star\n\nsubmission.head()","metadata":{"papermill":{"duration":0.120392,"end_time":"2021-03-08T07:59:11.127762","exception":false,"start_time":"2021-03-08T07:59:11.00737","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-12T21:36:51.675881Z","iopub.execute_input":"2022-04-12T21:36:51.676623Z","iopub.status.idle":"2022-04-12T21:36:51.689513Z","shell.execute_reply.started":"2022-04-12T21:36:51.676583Z","shell.execute_reply":"2022-04-12T21:36:51.688417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Look at the class distributions. \n# It can help to foresee when the classifier has really messed up before submitting to the platform\nsubmission['class'].value_counts() ","metadata":{"execution":{"iopub.status.busy":"2022-04-12T21:36:51.691082Z","iopub.execute_input":"2022-04-12T21:36:51.691492Z","iopub.status.idle":"2022-04-12T21:36:51.704066Z","shell.execute_reply.started":"2022-04-12T21:36:51.691386Z","shell.execute_reply":"2022-04-12T21:36:51.702998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv')","metadata":{"papermill":{"duration":0.122516,"end_time":"2021-03-08T07:59:11.356409","exception":false,"start_time":"2021-03-08T07:59:11.233893","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-12T21:36:51.705753Z","iopub.execute_input":"2022-04-12T21:36:51.706719Z","iopub.status.idle":"2022-04-12T21:36:51.720384Z","shell.execute_reply.started":"2022-04-12T21:36:51.706663Z","shell.execute_reply":"2022-04-12T21:36:51.719549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Discussion","metadata":{"papermill":{"duration":0.116655,"end_time":"2021-03-08T07:59:11.577703","exception":false,"start_time":"2021-03-08T07:59:11.461048","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"In summary we contributed the following: \n* Compare different face detection methods: \n    Haar face recognition and MTCNN, by comparing the results, we find MTCNN outperforms Haar face detection. For example, the image 0 in train set cannot be extracted well by using HAAR detector, but can be extracted via MTCNN.\n* Dataset preprocessing: \n    The accuracy of the final score can be improved if the bad extraced faces are removed, especially, for the nan-face detected images, they are assigned as black images, which affects the classifier performance. If the non-face images are removed, the accuracy can be improved by about 2%, if the blurred images are moved, the accuracy can be imporved by an extra 2%, indicating data cleaning as data preprocess is crucial in the classification task.\n* Apply different feature representations methods: \n    The discriminability of NN extractor is far better than the hand-crafted methods that we tried: HOG and PCA, verifying the powerful extracted ability of CNN that learns features and adapts to very difficult situation as opposed to traditional static computer vision methods.\n* Tranfer learning can improve the feature discriminability on specific traning sets. That is why we relied on facenet, a DNN that was trained on very large datasets for face regognition.\n* SVM can be used to perform the classification task, and achieve surprisingly good results.\n* We can further make use of the data at hand using the test set in combination with the train set for semi-supervised learning.","metadata":{}}]}